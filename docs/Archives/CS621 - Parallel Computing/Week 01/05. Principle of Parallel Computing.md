---
tags:
  - university-notes
university-name: Virtual University of Pakistan
date: 2025-12-05
---

# 05. Principle of Parallel Computing

<span style="color: gray;">Dated: 05-12-2025</span>

- Finding enough parallelism
- Scale
- Locality
- Load balance
- Coordination and Synchronization
- Performance modeling

## Finding Enough Parallelism

Conventional architectures coarsely comprise of a processor, memory system, and the data-path. Each of these components' present significant performance bottlenecks. Parallelism addresses each of these components in significant ways.

## Scale

Parallelism overhead includes cost of starting a head, accessing data, communicating shared data, synchronization and extra computation. Algorithms needs sufficiently large units of work to run fast in parallel.

## Locality

Parallel processors collectively have large and fast cache. The memory addresses are distributed across the processors, a processor may have faster access to memory locations mapped locally than to memory locations mapped to other processors.

## Load Balance

Determines the workload, divide up evenly before staring in case of static load balancing but in dynamic load balance workload changes dynamically, need to rebalance dynamically.

## Coordination and Synchronization

Several kind of synchronization is needed by cooperating perform to processes computation.

## Performance Modeling

More efficient programming models and tools for formulated massively parallel supercomputers.