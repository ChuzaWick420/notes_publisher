---
tags:
  - university-notes
university-name: Virtual University of Pakistan
date: 2025-06-17
---

# 34. Applications to Differential Equations

<span style="color: gray;">Dated: 17-06-2025</span>

## Differential Equations

A `differential equation` is any equation which contains `derivatives`,[^1] either ordinary `derivatives`[^1] or `partial derivatives`.[^2]

## System of `Linear Differential Equations`[^3]

A system of `linear differential equations`[^3] can be expressed as

$$
\begin{array}{ccccc}
	x_1^\prime &= &a_{11} x_1 &+ \ldots + &a_{1n}x_n \\
	x_2^\prime &= &a_{21} x_1 &+ \ldots + &a_{2n}x_n \\
	\vdots &&\vdots &&\vdots \\
	x_n^\prime &= &a_{n1} x_1 &+ \ldots + &a_{nn}x_n \\
\end{array}
$$

$$x^\prime = A x$$

Where

$$
x(t) = 
\begin{bmatrix}
	x_1(t) \\
	\vdots \\
	x_n(t) \\
\end{bmatrix}
$$

$$
x^\prime(t) = 
\begin{bmatrix}
	x_1^\prime(t) \\
	\vdots \\
	x_n^\prime(t) \\
\end{bmatrix}
$$

$$
A =
\begin{bmatrix}
	a_{11} & \cdots & a_{1n} \\
	\vdots & \ddots & \vdots \\
	a_{n1} & \cdots & a_{nn} \\
\end{bmatrix}
$$

## Superposition of Solutions

If $u$ and $v$ are solutions to $x^\prime = Ax$ then their `linear combination`[^4] (i.e. $au + bv$) is also a solution.  
This property is called `superposition` of solutions.

## Fundamental `Set`[^5] of Solutions

If $A$ is an $n \times n$ `matrix`[^6] with $n$ `linearly independent`[^7] `functions`[^8] then each solution to the equation $x^\prime = Ax$ is a `linear combination`[^4] of those `functions`[^8] and `set`[^5] of these `functions`[^8] is the `basis` for the `solution set` which is also $n$ dimensional `vector space`[^9] of `functions`.[^8]

## Initial Value Problem

If a `vector`[^9] $\vec x_0$ is given then the initial value problem is to construct a `function`[^8] $x$ such that $x^\prime = Ax$ and $x(0) = x_0$.

### Example

Find the solution to $x^\prime = Ax$ where

$$
A =
\begin{bmatrix}
	3 & 0\\
	0 & -5
\end{bmatrix}
$$

$$
\begin{bmatrix}
	x_1^\prime(t) \\
	x_2\prime(t)
\end{bmatrix}
=
\begin{bmatrix}
	3 & 0 \\
	0 & -5
\end{bmatrix}
\begin{bmatrix}
	x_1(t) \\
	x_2(t)
\end{bmatrix}
$$

$$\implies x_1^\prime(t) = 3x_1(t) \text{ and } x_2^\prime(t) = -5x_2(t)$$

$$\frac{d x_1(t)}{dt} = 3x_1$$

$$\frac {d x_1} {x_1} = 3dt$$

`Integrating`[^10] both sides

$$\int \frac{dx_1}{x_1} = 3 \int dt$$

 $$\ln x_1 = 3t + \ln c_1$$

 $$\ln x_1 - \ln c_1 = 3t$$

 $$\ln\left(\frac{x_1}{c_1}\right) = 3t$$

Taking `anti log`[^11] on both sides

$$\frac{x_1}{c_1} = e^{3t}$$

 $$x_1(t) = c_1 e^{3t}$$

Similarly, for $x^\prime_2(t) = -5x_2(t)$, solution will be

$$x_2(t) = c_2 e^{-5t}$$

$$
\begin{bmatrix}
	x_1(t) \\
	x_2(t)
\end{bmatrix}
=
\begin{bmatrix}
	c_1 e^{3t} \\
	c_2 e^{-5t}
\end{bmatrix}
=
c_1
\begin{bmatrix}
	1 \\
	0
\end{bmatrix}
	e^{3t} + c_2
\begin{bmatrix}
	0 \\
	1
\end{bmatrix}
e^{-5t}
$$

## Observation

General solution to $x^\prime = Ax$ might be of the form  

$$x(t) = v e^{\lambda t}$$

Multiply $A$ on both sides  

$$Ax(t) = Ave^{\lambda t}$$

where $\lambda$ is a scalar and $v$ is some non zero `vector`[^12]  
`Differentiating`[^1] original equation with respect to $t$.

$$x^\prime(t) = \lambda ve^{\lambda t}$$

$$x^\prime(t) = Ax(t) \iff \lambda v = Av$$

## Eigenfunctions

Each pair of `eigenvalue`[^13] and its corresponding `eigenvector` provides a solution of the equation $x^\prime = Ax$ which is called `eigenfunctions` of the `differential equation`.[^3]

## Decoupling a Dynamical System

Let a dynamical system be defined by $x^\prime = Ax$ where $A$ is `diagonalizable`.[^14]  
Suppose that [eigen functions](#eigenfunctions) for $A$ are  

$$\vec v_1e^{\lambda_1 t}, \ldots, \vec v_ne^{\lambda_n t}$$

where $\{\vec v_1, \ldots, \vec v_n\}$ are `linearly independent`[^7] and let $P = \begin{bmatrix}\vec v_1 & \cdots & \vec v_n\end{bmatrix}$ and $D$ be `diagonal matrix`[^15] with entries as $\lambda_1, \ldots, \lambda_n$ so that $A = PDP^{-1}$.  
Now replace $x(t) = Py(t)$ and substitute in $x^\prime = A x$ and in the end, we will get  

$$y^\prime(t) = Dy(t)$$

$$
\begin{bmatrix}
	y_1^\prime(t) \\
	y_2^\prime(t) \\
	\vdots \\
	y_n^\prime(t) \\
\end{bmatrix}
=
\begin{bmatrix}
	\lambda_1 & 0 & \cdots & 0 \\
	0 & \lambda_2 & \cdots & \vdots \\
	\vdots & \vdots & \ddots & 0\\
	0 & \cdots & 0 & \lambda_n
\end{bmatrix}
\begin{bmatrix}
	y_1(t) \\
	y_2(t) \\
	\vdots \\
	y_n(t) \\
\end{bmatrix}
$$

Change of variable from $x$ to $y$ has decoupled the system because now the `derivative`[^1] of each `function`[^8] $y_k$ depends only on $y_k$.  

$$\because y_1^\prime = \lambda_1y_1$$

$$\implies y_1(t)= \lambda_1 e^{\lambda_1 t}$$

$$
y(t) =
\begin{bmatrix}
	c_1 e^{\lambda_1 t} \\
	\vdots \\
	c_n e^{\lambda_n t}
\end{bmatrix},
\text{ where }
\begin{bmatrix}
	c_1 \\
	\vdots \\
	c_n
\end{bmatrix}
=
y(0) = P^{-1} x(0) = P^{-1} x_0
$$

To obtain the general solution $x$ of the original system, compute  

$$x(t) = Py(t) = \begin{bmatrix}v_1 & \dots & v_n\end{bmatrix}y(t) = c_1v_1 e^{\lambda_1 t} + \dots + c_nv_n e^{\lambda_n t}$$

This is the [eigenfunction](#eigenfunctions) expansion

## Complex Eigenvalues

Let $A$ be a `matrix`[^6] and if $\lambda$ is an `eigen value`[^13] then $\bar \lambda$ is the 2nd `eigen value`.[^13]  
Similarly, if $\vec v$ is an `eigen vector`[^13] then $\bar{\vec v}$ is the 2nd `eigen vector`.[^13]  
The solutions for $x^\prime = Ax$ are  

$$x_1(t) = ve^{\lambda t}, \ x_2(t) = \bar v e^{\bar \lambda t}$$

$$x_2(t) = \overline{x_1(t)}$$

$$\textbf{Re }(ve^{\lambda t}) = \frac 1 2 \left(x_1(t) + \overline{x_1(t)}\right)$$

$$\textbf{Im }(ve^{\lambda t}) = \frac 1 {2i} \left(x_1(t) - \overline{x_1(t)}\right)$$

$$\because e^x = 1 + x + \frac{x^2}{2!} + \dots + \frac{x^n}{n!} + \dots$$

$$e^{\lambda t} = 1 + \lambda t + \frac{(\lambda t)^2}{2!} + \dots + \frac{(\lambda t)^n}{n!} + \dots$$

$$\because \lambda = a + b \iota$$

$$e^{(a+b \iota)t} = e^{at} e^{\iota bt} = e^{at} (\cos bt + \iota \sin bt)$$

$$\therefore ve^{\lambda t} = (\textbf{Re } v + \iota \textbf{Im } v) \cdot e^{at} (\cos bt + \iota \sin bt)$$

After working it out, we will get

$$ve^{\lambda t} = e^{at}(\textbf{Re }v \cos(bt) - \textbf{Im }v \sin(bt)) + \iota e^{at}(\textbf{Re }v \sin(bt) - \textbf{Im }v \cos(bt))$$

Solutions of $x^\prime= Ax$ are

$$y_1(t) = \textbf{Re } x_1(t) = [\textbf{Re }v \cos(bt) - \textbf{Im }v \sin(bt)]e^{at}$$

$$y_2(t) = \textbf{Im } x_1(t) = [\textbf{Re }v \sin(bt) + \textbf{Im }v \cos(bt)]e^{at}$$

## References

Read more about [[M_Notations|notations and symbols]].

[^1]: Read more about [[mth101_15|derivatives]].
[^2]: Read more about [[mth301_06|partial derivatives]].
[^3]: Read more about [[mth401_07|linear differential equation]].
[^4]: Read more about [[mth501_09|linear combinations]].
[^5]: Read more about [[M_Set|sets]].
[^6]: Read more about [[mth501_02|matrices]].
[^7]: Read more about [[mth501_08|linear independence]].
[^8]: Read more about [[M_Function|functions]].
[^9]: Read more about [[mth501_20|vector spaces]].
[^10]: Read more about [[mth101_25|integration]].
[^11]: Read more about [[M_Logarithm|logrithms]].
[^12]: Read more about [[mth301_10|vectors]].
[^13]: Read more about [[mth501_28|eigen values and vectors]].
[^14]: Read more about [[mth501_30|diagonalization]].
[^15]: Read more about [[mth501_11|diagonal matrices]].